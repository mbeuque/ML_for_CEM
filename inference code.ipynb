{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\manon37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\manon37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\manon37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\manon37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\manon37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\manon37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "import os\n",
    "\n",
    "# import keras_retinanet\n",
    "from keras_maskrcnn import models\n",
    "from keras_maskrcnn.utils.visualization import draw_mask\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption, draw_annotations\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "from keras_retinanet.utils.gpu import setup_gpu\n",
    "\n",
    "from keras_maskrcnn.bin.train import create_models\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import keras_retinanet\n",
    "\n",
    "from keras_maskrcnn.preprocessing import csv_generator\n",
    "import pandas as pd\n",
    "\n",
    "from keras_retinanet.utils.transform import random_transform_generator\n",
    "from keras_retinanet.utils.image import random_visual_effect_generator\n",
    "\n",
    "from keras_maskrcnn.bin.train import create_callbacks\n",
    "from keras_maskrcnn.bin.train import parse_args\n",
    "from keras_retinanet.callbacks import RedirectModel\n",
    "from keras_maskrcnn.callbacks.eval import Evaluate\n",
    "from keras_retinanet.models.retinanet import retinanet_bbox\n",
    "\n",
    "# import miscellaneous modules\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "keras.backend.tensorflow_backend._get_available_gpus()\n",
    "import keras_maskrcnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re,random,time,sys\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage.measure import regionprops,label\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "from PIL import Image\n",
    "from skimage import exposure\n",
    "import random \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define example paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_low_energy= r\"example input\\example_img_low_energy.mha\"\n",
    "path_input_recombined = r\"example input\\example_img_recombined.mha\"\n",
    "path_weights = r\"resnet101_16.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(img_read_re, img_read_le):\n",
    "    otsu = sitk.OtsuThresholdImageFilter()\n",
    "    otsu_image = otsu.Execute(img_read_re)\n",
    "    otsu_array = sitk.GetArrayFromImage(otsu_image)\n",
    "    \n",
    "    invert_otsu = (np.ones(otsu_array.shape)-otsu_array).astype(np.uint8)\n",
    "    (contours,_) = cv2.findContours(invert_otsu, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    max_ctr = contours[0]\n",
    "    for ctr in contours:\n",
    "        if cv2.contourArea(ctr) > cv2.contourArea(max_ctr):\n",
    "            max_ctr= ctr\n",
    "    img_temp = np.zeros(invert_otsu.shape)\n",
    "    polygon = ctr\n",
    "    cv2.fillPoly( img_temp, [max_ctr], [1] )\n",
    "    otsu_array = np.ones(invert_otsu.shape)-img_temp\n",
    "        \n",
    "    temp_img_re = sitk.GetArrayFromImage(img_read_re)\n",
    "    \n",
    "    temp_img_le_original = sitk.GetArrayFromImage(img_read_le)\n",
    "        \n",
    "    temp_img_re =(np.ones((img_read_re.GetSize()[1], img_read_re.GetSize()[0]))-otsu_array)*temp_img_re\n",
    "    temp_img_le =(np.ones((img_read_le.GetSize()[1], img_read_le.GetSize()[0]))-otsu_array)*temp_img_le_original\n",
    "    props = regionprops(np.array(temp_img_re>0,np.uint8))\n",
    "    r0, c0, r1, c1 = props[0].bbox\n",
    "    temp_img_re = temp_img_re[r0:r1, c0:c1]\n",
    "    \n",
    "    temp_img_re = pre_processing_for_img(temp_img_re)\n",
    "    \n",
    "    temp_img_le = temp_img_le[r0:r1, c0:c1]\n",
    "    \n",
    "    temp_img_le = pre_processing_for_img(temp_img_le)\n",
    "    \n",
    "    return temp_img_re, temp_img_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_intensities(orig_img,bin_nr=256):      \n",
    "    v_count=0\n",
    "    img_list=[]\n",
    "    filtered = orig_img.copy()\n",
    "    if np.min(orig_img.flatten())<0:\n",
    "        filtered+=np.min(orig_img.flatten())\n",
    "    resampled = np.zeros_like(filtered)\n",
    "    max_val_img = np.max(filtered.flatten())\n",
    "    min_val_img = np.min(filtered.flatten())\n",
    "    step = (max_val_img-min_val_img)/bin_nr\n",
    "\n",
    "    for st in np.arange(step+min_val_img,max_val_img+step,step):\n",
    "        resampled[(filtered<=st)&(filtered>=st-step)] = v_count\n",
    "        v_count+=1\n",
    "    \n",
    "    return np.array(resampled,dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_for_img(img):\n",
    "    temp_img = img.copy()\n",
    "    low_thr = np.quantile(temp_img[temp_img>0], 0.01)\n",
    "    high_thr = np.quantile(temp_img[temp_img>0], 0.99)\n",
    "    temp_img[temp_img<low_thr] = low_thr\n",
    "    temp_img[temp_img>high_thr] = high_thr\n",
    "    if len(np.unique(temp_img[temp_img>0])) > 256:\n",
    "            temp_img_sampled = resample_intensities(temp_img[temp_img>0])\n",
    "            temp_img[temp_img>0] = temp_img_sampled\n",
    "    else:\n",
    "        new_img =(temp_img-np.min(temp_img))/(np.max(temp_img)-np.min(temp_img)) \n",
    "        temp_img = (new_img*255).astype(np.uint8)\n",
    "    return temp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patient(path_input_recombined,path_input_low_energy):\n",
    "    img_read_re = sitk.ReadImage(path_input_recombined)\n",
    "    img_read_le = sitk.ReadImage(path_input_low_energy)\n",
    "    img_re, img_le = crop_img(img_read_re, img_read_le)\n",
    "    return img_le,img_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path_input_low_energy,path_input_recombined):\n",
    "    clahe = cv2.createCLAHE(clipLimit =2.5, tileGridSize=(16,16))\n",
    "    clahe_recombined_2 = cv2.createCLAHE(clipLimit =1.0, tileGridSize=(16,16)) \n",
    "\n",
    "    img_le, img_re = load_patient(path_input_recombined,path_input_low_energy)\n",
    "    new_img_le =(img_le-np.min(img_le))/(np.max(img_le)-np.min(img_le)) \n",
    "    im_le = (new_img_le*255).astype(np.uint8)\n",
    "\n",
    "    x_le = clahe.apply(im_le).astype(np.uint8)\n",
    "    new_img_re =(img_re-np.min(img_re))/(np.max(img_re)-np.min(img_re)) \n",
    "    new_img_re = (new_img_re*255).astype(np.uint8)\n",
    "\n",
    "    x_re = clahe.apply(new_img_re).astype(np.uint8)\n",
    "    x_re_2 = clahe_recombined_2.apply(new_img_re).astype(np.uint8)\n",
    "\n",
    "    temp_im_le = Image.fromarray(x_le)\n",
    "    temp_im_re = Image.fromarray(x_re)\n",
    "    temp_im_re_2 = Image.fromarray(x_re_2)\n",
    "    merged_img =Image.fromarray( cv2.merge((x_le,x_re,x_re_2)) )\n",
    "    img_rgb = merged_img.convert(\"RGB\")\n",
    "    img_bgr = np.asarray(img_rgb)[:, :, ::-1].copy()\n",
    "    return img_bgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_classify_lesions(path_input_recombined,path_input_low_energy,path_weights):\n",
    "    \n",
    "    img_bgr = preprocessing(path_input_low_energy,path_input_recombined)\n",
    "    \n",
    "    backbone = models.backbone(\"resnet101\")\n",
    "    model, training_model, prediction_model = create_models(backbone_retinanet=backbone.maskrcnn, num_classes=2, weights=path_weights , freeze_backbone=False)   \n",
    "    \n",
    "    mask_prediction = []\n",
    "    dict_prediction = {}\n",
    "    labels_to_names = {0:'benign',1:'malignant'}\n",
    "    image = img_bgr.copy()\n",
    "    image = preprocess_image(image)\n",
    "    image, scale = resize_image(image)\n",
    "\n",
    "    outputs = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "    boxes  = outputs[-4][0]\n",
    "    scores = outputs[-3][0]\n",
    "    labels = outputs[-2][0]\n",
    "    masks  = outputs[-1][0]\n",
    "    boxes /= scale\n",
    "\n",
    "    selected_indices = tf.image.non_max_suppression(tf.constant(boxes.reshape(-1,4)), tf.constant(scores.flatten()), max_output_size=tf.constant(5), iou_threshold=0.01)\n",
    "    selected_boxes = tf.gather(tf.constant(boxes.reshape(-1,4)), selected_indices)\n",
    "    session = tf.Session()\n",
    "    with session.as_default():\n",
    "        boxes = selected_boxes.eval() \n",
    "\n",
    "    for box, score, label, mask in zip(boxes, scores, labels, masks):\n",
    "        if score > 0.1 and box[0] > 0 and box[1]>0 and box[2] > 0 and box[3]>0: \n",
    "            b = box.astype(int)\n",
    "            dict_prediction[str(b)]=labels_to_names[label]\n",
    "            \n",
    "            temp_mask = mask[:, :, label] ##warning: the mask return is within the bounding box and has a fix size of 28x28\n",
    "            mask_prediction.append(temp_mask)\n",
    "            \n",
    "    return np.array(mask_prediction),dict_prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
